#### 机器学习相关知识点：优化算法、决策树

##### 支持向量机SVM：

* **SVM学习的基本思想是求解能够正确划分训练数据集并且几何间隔最大的分割超平面【实际上只有几个点共同确定了超平面的位置，支持向量（支持向量对应的样本点到决策面的距离的两倍）】**

  

* **对于有约束条件的凸优化问题，通过拉格朗日乘子法把等式约束条件与目标函数进行合并。通过拉格朗日函数对各个变量求导，令其为零，可以求得候选值集合，然后验证求得最优值。**

* **KKT条件：通过一些条件，可以求出最优值的必要条件。【可行解域外、可行解域内】**

  * **拉格朗日对偶：使用拉格朗日获得的函数，使用求导的方法求解依然困难。**
    * **将有约束的原始目标函数转换为无约束的新构造的拉格朗日目标函数**
    * **使用拉格朗日对偶性，将不易求解的优化问题转化为易求解的优化**
  * **建立一个在可行解区域内与原目标函数相同，在可行解区域外函数值趋近无穷大的心函数：**
    * ![image-20220124150126225](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220124150126225.png)
    * ![image-20220124150134124](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220124150134124.png)
  * **将不易求解的优化问题转化为易求解的优化：拉格朗日对偶性问题，将极小极大问题—>极大极小问题**
    * **【对偶问题的目标函数】**![image-20220124151023189](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220124151023189.png)
    * **【原问题的目标函数】**![image-20220124151126882](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220124151126882.png)

  * **KKT条件：使用拉格朗日对目标函数进行处理，生成了一个新的目标函数，通过一些条件，可以求出最优解的必要条件。**
    * **只有原始问题和对偶问题都满足KKT条件，那么原始问题的最优解和对偶问题的最优解相等**
  * **对偶问题求解**
    * **首先要让L(w, b, a)关于w和b最小化，回代入目标函数**
    * **然后求对a的极大**
    * **最后利用SMO算法求解对偶问题中的拉格朗日乘子，回代入求解w、b**

* **松弛变量：允许有些数据点可以处于超平面的错误的一侧**

* **非线性SVM和核函数：**

  * **通过一个非线性变换将输入空间（如欧式空间）对应到一个特征空间（希尔伯特空间），得到一个非线性问题转换成一个线性可分的问题**

##### 集成学习

* **boosting集成学习**
  * 串行的方式训练基分类器，各分类器之间有依赖。每次训练时，对前一层基分类器分错的样本给与更高的权重。【不断修正模型的预测误差】

* **bagging集成学习**
  * 各个分类器之间无强依赖，可以并行。【通过数据采样处理】
* **方差&偏差**
  * **偏差是指由所采样得到的大小为m的训练数据集，训练出的所有模型的输出的平均值和真实模型输出之间的偏差。【描述模型输出结果的期望与样本真实结果的差距】【拟合能力，欠拟合】**
  * **方差是指所有采样的大小为m的训练数据集，训练出的所有的模型输出的方差【描述模型对于给定值的输出稳定性，分类器对样本分布过于敏感，到指在训练样本较少的时候，出现过拟合】【泛化能力，过拟合】**
  * 基分类器的错误，是偏差和方差之和。
  * boosting方法通过逐步聚焦分类器分错的样本，减少集成分类器的偏差
  * Bagging采用分而治之的策略，通过对样本多次采样，分别训练多个模型，减少方差
  * **为什么决策树是常用的基分类器：**
    * 可以方便地将样本权重整合到训练过程中，不需要使用过采样的方法来调整样本权重。
    * 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中
    * 数据样本扰动对决策树影响较大，因此不同子样本集生成的基分类器随机性就较大。这个的不稳定学习器更适合作为基分类器。
* **Adaboost模型**
  * 对分类正确的样本降低权重
  * 对错误分类的样本升高或者保持权重不变
  * 在模型融合过程中，根据错误率对基分类器进行加权融合，错误率低的分类器拥有更大的权重。
* **GBDT模型**
  * 拟合：负梯度方向
  * 损失函数：
    * 回归：直接用连续的值计算负梯度
      * 均方损失
      * 绝对损失
    * **分类：指数，sigmoid激活函数**
  * **GBDT模型的优点：**
    * 可以灵活处理各种类型的数据，包括连续值和离散值
    * 在相对少的调参时间情况下，预测的准确率也可以比较高。
    * 使用一些健壮的损失函数，对异常值的鲁棒性非常强。
    * 预测阶段计算速度较快，树与树之间可以并行化计算
    * 在分布稠密的数据集上，泛化能力和表达能力都比较好
    * 具有较好的解释性和鲁棒性
    * 能够自动发现特征之间的高阶关系
    * 不需要做特殊化处理（归一化）
  * **GBDT模型的缺点：**
    * 由于弱学习器之间存在依赖关系，难以并行训练数据。
    * 在高维系数数据上，表现不如SVM或神经网络
    * 在处理文本分类特征问题上，相对其他模型优势不如在处理数据特征时明显
    * 在训练过程需要串行，只能在决策树内部采用一些局部并行手段提高训练速度
* **XGboost模型**
  * 