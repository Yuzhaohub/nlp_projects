#### 集成树模型：GBDT、XGboost、LightGBM

##### GBDT模型

* **残差与梯度的关系：**

  * 基于残差的gbdt是一种特殊的gbdt模型，它的损失函数是平方损失函数，常用来处理回归问题。
  * 算法的损失函代表模型的偏差面，最小化损失函数，相当于最小化模型的偏差；模型的方差代表模型的泛化能力，在目标函数包括抑制模型复杂度的正则项。
    * **若基模型是树模型，则树的深度、叶子节点树等指标可以反应树的复杂程度。**

* **GBDT与XGboost模型损失函数**

  * **目标损失函数：只需要求出每一步损失函数的一阶和二阶导的值，然后最优化目标函数，就可以得到每一步的f（x），最后根据加法模型得到一个整体模型**

    * $$
      Obj^{(t)}=\sum^n_{i=1}l(y_i,\bar{y_i}) + \sum^t_{i=i}\Omega(f_i)
      \\ =\sum^n_{i=1}l(y_i,\bar{y_i}^{t-1} + f_t(x_i)) + \Omega(f_t)+constant
      $$

  * **泰勒展开式**

    * $$
      f(x + \Delta x)≈ f(x) + f'(x) \Delta x + \frac{1}{2}f^"(x)\Delta x^2
      $$

  * ![image-20220120175830854](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220120175830854.png)

* **在GBDT中都是回归树，即分类问题转化成对概率的回归**
  * **q(x)表示每个样本在哪个叶子节点上，wq则代表哪个叶子节点去什么w值，代表每个样本的取值w（即预测值）**
    * ![image-20220120180824083](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220120180824083.png)
  * ![image-20220120180904432](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220120180904432.png)
  * ![image-20220120182825521](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220120182825521.png)
  * **Ij = {i|q(xi) = j}为第j个叶子节点的样本集合**
    * ![image-20220120182835097](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220120182835097.png)
  * ![image-20220120183054408](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220120183054408.png)
* **如何最优化目标函数**
  * **算法在拟合的每一步都新生成一颗决策树**
  * **在拟合这棵树之前，需要计算损失函数在每个样本上的一阶导和二阶导，即gi和hi**
  * **通过把上面的贪心策略生成一棵树，计算每个叶子结点的Gj和Hj，预测值w**
  * **把新生成的决策树ft(x)，加入学习率，主要为了抑制模型的过拟合。**
* **GBDT的残差为什么用负梯度替代**
  * **无论损失函数是什么，每个决策树拟合的都是负梯度，不是用负梯度代替残差，而是当损失函数是均方损失时，负梯度刚好是残差，残差只是特例。**
  * **GBDT模型思想：迭代一个加法模型，在不改变上一轮模型的情况下，找到一个新的基学习器使得损失最小**
    * **存在一个优化问题：要求得到一个函数（新加入的基函数），使得损失函数达到最小值。为了求解这个优化问题，把函数当做特殊的"参数"，用“梯度下降”的方法求解。**
    * **梯度下降：就是要求负梯度方向（导数），当损失函数不是平方损失时，函数的负梯度就不等于残差**
    * **通过目标损失函数的一阶泰勒展开式：第k棵树本质就是在拟合当前的负梯度**

##### GBDT模型知识点

* **为什么GBDT中要拟合残差？**
  * **首先，GBDT拟合的不是残差，而是负梯度。只有当损失函数为平方损失的时候，负梯度正好为残差。**
  * **GBDT通过弱学习器的累加实现强学习器，每个弱学习器需要不断减少损失函数，即每加一个树都应该减少损失函数**
    * ![image-20220121110033648](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220121110033648.png)
* **GBDT如何做分类？**
  * **首先要明确，GBDT内部每棵树都是回归树，不论是回归还是分类问题。在预测的数值基础上，引入sigmoid激活函数，将其映射都0-1概率值**
  * ![image-20220121110342290](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220121110342290.png)
* **GBDT、XGBoost、LightGBM的区别**
  * **XGboost使用二阶泰勒展开表示梯度，即每棵树拟合的是二阶泰勒的梯度，相比GBDT的一阶泰勒展开，对梯度的表示更准确。**
  * **XGboost的损失函数中显示加入了正则项，对叶子数目和叶子权重做惩罚，更好抑制过拟合**
  * **LightGBM对遍历每个特征寻找最有分裂点，对每个特征进行了分桶（直方图），比如可指定分为64个捅，那么该特征所有的值都落入这64个捅中，遍历这个特征时，最多只需要遍历64次，则每次分裂的复杂度为O(特征数*桶数)，如果不分桶，则可能所有样本的值都不同，则复杂度为O(特征数*样本数)**
    * **每棵树都是弱分类器，不需要非常精确，且分桶一定程度上提高了泛化能力，降低了误差**
  * **LightGBM的分枝模式为leaf-wise，即遍历当前所有待分枝节点，不需要一定在最下边一层，谁的分裂增益大就分谁。而XGboost的分支模式为level-wise，即分完一层在分下一层，可能一层中有些叶子分裂增益极小，但是仍然花费时间和空间去分裂。**

##### AdaBoost模型知识点

* **前一个基本分类器被错误分类的样本的权值会增大，而正确分类的样本的权值会减少，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或者预先指定的最大迭代次数才确定最终的强分类器。**

* **Adaboost算法步骤：**

  * **首先，是初始化训练数据的权值分布D1，假设有N个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值：w1 = 1/N**
  * **然后，训练弱分类器hi。具体训练过程中：如果某个训练样本点，被弱分类器hi准确分类，那么在构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。**
  * **最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起到较大的决定作用，从而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。【误差率低的弱分类器在最终分类器中占的比重较大，否则较小。】**
  * **【核心】：弱学习器权重系数、样本权重系数**
  * **【模型公式推导】分布求解导数，循环代入求解。**

* **Adaboost算法流程**

  * **初始化样本集权重**

    * ![image-20220121162217021](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220121162217021.png)

  * **使用具有权重Dk的样本集来训练数据，得到弱分类器Gk(x)**

  * **计算Gk(x)的分类误差率**

  * **计算弱分类器的系数**

  * **更新样本集的权重分布**

  * **构建最终分类器**

    ![image-20220121162949031](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220121162949031.png)

    