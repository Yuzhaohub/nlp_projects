#### 词向量模型

* **Word2Vec中skip-gram是什么，Negative Sampling怎么做？**

  * Word2Vec通过学习文本然后用词向量的方式表征词的语义信息，然后使得语义相似的单词嵌入式空间中的距离很近。而在word2vec模型中有skip-gram和CBOW两种模式。
    * Skip-Gram是给定输入单词来预测上下文：**CBOW是输入词向量求平均**
    * CBOW是给定上下文来预测输入单词：**输入只有一个，不要求平均**
  * Negative Sample是对给定的词，并生成其负采样词集合的一种策略，已知有一个词，**这个词可以看做一个正例，而它的上下文词集可以看做是负例**，但是负例的样本太多了，**而在预料库中，各个词出现的概率是不一样的，所以在采样时可以要求高频词选中的概率较大，低频词选中的概率较小。**
    * **负采样的思想：不直接让模型从整个词表找最可能的词，而是直接给定这个词（即正例）和几个随机采样的噪声词（即采样出来的负例），只要模型能从这里找出正确的词就认为完成目标**
    * **负采样重点：**
      * ![image-20220225144314586](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220225144314586.png)
      * **将多分类问题转换为k+1个二分类问题，从而减少计算量，加快训练速度。**
      * **保证模型训练效果，因为目标词只跟相似的词有关，没有必要使用全部的单词作为负例来更新它们的权重。**
  * **层次Softmax：**
    * **softmax需要对每个词语都计算输出概率，并进行归一化，计算量很大。进行softmax的目标是多分类，可以转换成多个二分类问题。**
      * 用huffman编码做词表示
      * 把N分类变成log(N)个2分类。每个二分类上用二元逻辑回归的方法（sigmoid）

* **Word2Vec整体流程：**

  * Skip-Gram实际上分为两部分，**第一部分为建立模型，第二部分为通过模型获取嵌入词向量。**
  * **word2vec的整个建模过程实际上与自编码器相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，【需要将这个模型通过训练所学到的参数，如隐含层的权值矩阵，即为词向量】**
  * **自编码：**通过在隐含层将输入进行编码压缩，继而在输出层将数据解码恢复到初始状态，训练完成后，将会去掉输出层，只保留隐层。
  * **Skip-Gram模型流程：**
    * 首先选取句子中的一个词作为输入词**（实际训练过程中，是依次将句子中的每个词作为输入词构建训练对的）**
    * **skip_window和num_skips：**包括input word在内
      * skip_window：表示从当前input word的一侧选择词的数量。
      * num_skips：表示选取多少个不同的词作为output word，以（input word， output word）
    * 神经网络基于这些训练数据将会输出一个概率分布，这个概率代表词典中每个词是output word的可能性。

* **为什么Word2vec中会用到负采样？**

  * 动机：使用huffman树来替代传统的神经网络，可以提高模型训练的效率。但是如果我们的训练样本里的中心词w是个很生僻的词，那么就得在huffman中辛苦查找。
  * 介绍：一种概率采样的方式，可以根据词频进行随机抽样，倾向于词频较大负样本。
  * 优点：
    * 用来提高训练速度并且改善所得到词向量的质量。
    * 不同于原本每个训练更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样会降低梯度下降过程中的计算量。
  * **极大化正样本出现的概率，同时极小化负样本出现的概率，以sigmoid来替代softmax，相当于进行二分类，判断这个样本是不是正样本**

* **word2vec和ti-idf在相似度计算时的区别？**

  * word2vec是稠密的向量，而tf-idf则是稀疏的向量
  * word2vec的向量纬度一般远比tf-idf的向量纬度小很多
  * word2vec的向量可以表达语义信息，但是tf-idf的向量不可以
  * word2vec可以通过计算余弦相似度来得出两个向量的相似度，但是tf-idf不可以。

* **word2vec训练trick，window设置多大？**

  * 比较大，会提取更多的topic信息。
  * 设置比较小的话会更加关注于词本身。
  * 默认参数为5，但是在有些任务中window为2效果最好

* **word2vec训练trick，词向量维度，大与小有什么影响，还有其他参数？**

  * 词向量维度代表了词语的特征，特征越多能够更准确地将词与词区分，就好像一个人特征越多越容易与他人区分开来。但是在实际应用中维度太多训练出来模型会越大，虽然维度越多能够更好区分，但是词与词之间的关系也会被淡化，这与我们训练词向量的目的是相反。**训练词向量是希望能够通过统计来找出词与词之间的联系，维度太高会淡化词之间的关系，但是维度太低又不能将词区分【一般来说200-400比较常见】**

  

  

#### TF_IDF词向量模型

* TF-IDF：是一种统计方法，用以评估句子中的某个词（字）对于整个文档的重要程度。
* **TF-IDF如何评估词的重要程度？**
  * **对于句子中的某一个词（字）随着其在整个句子中的出现次数的增加，其重要性也随着增加；（正比关系）【体现词在句子中频繁性】**
  * **对于句子中的某一个词（字）随着其在整个文档中的出现频次的增加，其重要性也随着减少；（反比关系）【体现词在文档中的唯一性】**
* **TF-IDF的思想：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为词或者短语具有很好的类别区分能力，适用用来分类。**





#### Elmo

* **预训练时，适用语言模型学习一个单词的embedding（多义词无法解决）**
* **使用时，单词间具有特定上下文，可根据上下文单词语义调整单词的embedding表示（可解决多义词问题）**
  * **理解：**因为预训练过程中，emlo中的lstm能够学习到每个词对应的上下文信息，并保存在网络中，在fine-tuning时，下游任务能够对网络进行fine-tuning，使其学习到新特征。



#### BERT

* **Bert的具体网络结构，以及训练过程，bert为什么会火，它的改进？**

  * bert使用了transformer的encoder侧的网络，作为一个文本编码器，预训练使用两个loss，**一个是mask LM**，遮蔽掉源端的一些字**（mask的具体操作：15%概率mask词，这其中80%用【MASK】替换，10%随机替换成一个其他字，10%不替换）**，然后根据上下文去预测这些字；**一个是next sentence**，判断两个句子是否在文章中互为上下句，然后使用大规模的预料去预训练。

* **Bert预训练任务Masked LM存在问题：**

  * **预训练和微调之间的不匹配：在微调期间从未看到【MASK】词块**

* **讲讲Multi-Head attention的具体结构？**

  * BERT由12层transformer layer（encoder）构成，首先word emb、pos emb**（哪几种position embedding的方式）**，sent emb做加和作为网络输入，每一层由一个multi-head attention，一个feed forward以及两层layerNorm构成

    * **step 1：**

      * 一个768的hidden向量，被映射成query、key、value。**将768拆分为12个小的64维的向量，每一组小向量之间做attention**，不妨假设batch_size为32，seq_len为512，隐层维度为768,12个head。

      ```python
      # query(32 x 12 x 512 x 64)
      hidden(32x512x768)->query(32x512x768)->32x12x512x64
      # key(32 x 12 x 512 x 64)
      hidden(32x512x768)->query(32x512x768)->32x12x512x64
      # value(32 x 12 x 512 x 64)
      hidden(32x512x768)->query(32x512x768)->32x12x512x64
      ```

    * **step 2：**

      * 然后query和key之间做attention，得到一个32x32x512x512的权重矩阵，然后根据这个权重加权value中切分好的向量，得到一个32x12x512x64的向量，拉平输出为768向量。

```python
# 计算权重矩阵：Q*K^T
32x12x512x64(query) * 32x12x64x512(key^T) -> 32 x 12 x 512 x 512
# 加权求和：V^T*(Q*K^T)
32x12x64x512(value^T) * 32x12x512x512(Query*Key^T)->32x12x512x64
# 多头矩阵拉伸平整
32 x 12 x 512 x 64 -> 32 x 512 x 12 * 64 -> 32 x 512 x 768
```

* **Bert采用哪种Normalization结构，LayerNorm和BatchNorm区别，LayerNorm结构有参数吗，参数的作用？**
  * 采用LayerNorm结构，和BatchNorm的区别主要是做规范化的维度不同，**BatchNorm针对一个batch里面的数据进行规范化，针对单个神经元进行（比如batch里面有64个样本，那么规范化输入的这64个样本各自经过这个神经元后的值（64维））**，**LayerNorm则是针对单个样本，**





