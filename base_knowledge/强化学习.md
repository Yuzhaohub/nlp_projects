#### 强化学习

​	强化学习和监督学习最大的区别它没有监督学习已经准备好的训练数据输出值，强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，**它不是事先给出来的，而是延后给出的。强化学习的每一步与时间顺序前后关系紧密，而监督学习的训练数据之间一般都是独立的，没有这种前后的依赖关系。**

​	强化学习和非监督学习的区别：**非监督学习是没有输出值也没有奖励值的，它只有数据特征。同时和监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系。**

* **强化学习要素：**
  * 环境的状态S
  * 个体的动作A
  * 环境的奖励R
  * 个体策略（policy）：代表个体采取动作的依据，即个体会依据策略π来选择动作
  * 行动后的价值（value）：这个价值一般是个**期望函数**（虽然当前动作会给一个延时奖励，但是光看这个延时奖励是不行的，因为当前的延时奖励高，不代表后续的奖励也高）
    * ![image-20220224161332175](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220224161332175.png)
    * 奖励衰减因子（γ）：在[0, 1]之间，如果为0，则是贪婪法，即价值只由当前延时奖励决定；如果为1，则所有的后续状态奖励和当前奖励一视同仁。**（大多情况下，会取0-1之间的数字，即当前延时奖励的权重比后续奖励的权重大）**
  * 环境状态转移模型：**可以理解为一个概率状态机，即在状态s下采取动作a转到下一个状态的概率**
  * 探索率：会有一定的概率（探索率）不选择使当前轮迭代价值最大的动作，而选择其他的动作。

* **马尔科夫决策过程：**

  * 马尔科夫假设：
    * 环境的状态转移模型：只与前一个状态有关
    * 个体的策略：只与当前状态有关
    * 价值函数（采取行动后的reward）：只与当前状态有关
    * **动作价值函数：**
      * ![image-20220224164427577](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220224164427577.png)

* **动态规划与强化学习问题的联系**

  * **强化学习的两个基本问题：**
    * **第一个问题**：
      * 即给定强化学习的6个要素：状态集、动作集、模型状态转化概率矩阵、即时奖励，衰减因子给定策略π，求解该策略的状态价值函数
    * **第二个问题：**
      * 求解最优的价值函数和策略。给定强化学习的5个要素：状态集、动作集、状态转化概率矩阵、即时奖励、衰减因子，求解最优的状态价值和最优策略。
    * **策略评估**
      * **策略评估：从任意一个状态价值函数开始，依据给定的策略，结合贝尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，直至其收敛，得到该策略下最终的状态价值函数。**
      * ![image-20220224171316606](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220224171316606.png)
  * **策略迭代求解控制问题：**
    * **根据我们之前基于任意一个给定策略评估得到的状态价值来及时调整我们的动作策略（策略迭代）**
      * 贪婪法：**个体在某个状态下的行为是其能够达到后续所有可能的状态价值最大的那个状态。**
        * 第一步是使用当前策略π评估计算当前策略的最终状态价值
        * 第二步是根据状态价值v（贪婪法）更新策略π，
        * 接着回到第一步，一直迭代下去，最终得到收敛的策略π和状态价值v
      * **我们没有等到状态价值收敛才调整策略，而是随着状态价值的迭代及时调整策略，这样可以大大减少迭代次数。**

* **用蒙特卡洛法（MC）求解**

  * **特点：**

    * 模型状态转化概率矩阵未知，即不满足马尔科夫性质。
    * **蒙特卡洛通过采样若干经历完整的状态序列来评估状态的真实价值。（所谓的经历完整，就是这个序列必须是达到终点的）**
    * 蒙特卡洛的特点：
      * 1、与动态规划相比，它不需要依赖于状态转移概率
      * 2、它是从经历的完整序列学习，完整的经历越多，学习效果越好。

  * **蒙特卡洛法控制问题算法流程：**

    * 输入：状态集S，动作集A，即时奖励，衰减因子，探索率

    * 输出：最优的动作价值函数q和最优策略

      ![image-20220224182912760](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220224182912760.png)

    * **蒙特卡洛法是第二个求解强化问题的方法，是不基于模型的强化问题求解方法。它可以避免动态规划求解过于复杂，同时还可以不事先知道环境转化模型，因此可以用于海量数据和复杂模型。【每次采样都需要一个完整的状态序列】**