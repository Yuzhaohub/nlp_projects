#### nlp相关知识点回顾：

##### 激活函数

在深度学习中，输入值和矩阵的运算是线性的，而多个线性函数的组合仍然是线性函数，对于多个银行曾的神经网络，如果每一层都是线性函数，那么这些层在做的只是线性计算。**为模型提供非线性表征，使得模型可以表达的形式更多**

* sigmoid激活函数：
  * 输入值与权重矩阵进行计算得到的输出值范围很大，sigmoid将输出值压缩到0-1之间，表示某个分类的概率，**通常用于二分类**
  * ![image-20211217104303682](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20211217104303682.png)

* tanh激活函数**（双曲正切函数）**
  * 双曲正切函数tanh和sigmoid类似，区别在于**输出值范围由（0,1）—> （-1， 1）**
  * ![image-20211217104551881](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20211217104551881.png)
  * **隐藏层使用tanh函数的效果总体要优于sigmoid函数的，因为函数值域在-1到+1之间的激活函数，输出是以零为中心。如果使用tanh函数代替sigmoid函数，就会使得输出数据的平均值更接近0而不是0.5。即输入为负数的话，输出也为负数；同理输入为正数，输出也为正数。**
  * **缺点：**sigmoid和tanh有相同的缺点，**在值非常大的时候，函数的导数会变得特别小，甚至接近0**，使得梯度下降的速度减缓，导致模型学习的效率降低**（梯度消失）**。
* Relu激活函数**（修正线性单元的函数）**
  * 修正线性单元的函数Relu的函数表达式为**relu=max(0, x)**，就是取x和0的最大值。
  * **优缺点：**没有指数运算等耗费计算资源的操作，Relu不会出现梯度消失的问题（非0即1），对梯度下降的收敛有加速作用。**在实践中，使用Relu激活函数的神经网络通常会比使用sigmoid或者tanh激活函数学习更快更好。**
* Leaky Relu激活函数：
  * Leaky Relu是Relu的改进版本，当输入值是负值时，函数值不等于0，给了一个很小的负数梯度值，a通常为0.1。**这个函数通常比Relu激活函数效果好，但是效果不稳定，在实际中Leaky Relu使用并不多**
  * **理解：**（修正线性单元激活函数）Relu对所有负值输入后立即变为零，**降低了模型根据进行适当拟合或训练的能力，这意味着任何给Relu激活函数的负输入都会立即变成零。**当Relu进行负半区的时候，梯度为0，神经元此时不会训练，就会产生稀疏性。
* softmax激活函数：
  * **比较：**在二分类任务中，输出层使用的激活函数为sigmoid，而对于多分类的情况，需要用到softmax激活函数给每一个类都分配一个概率。
  * **softmax函数将每个单元的输出压缩到0到1之间，是标准化输出，**输出之和等于1。**softmax函数的输出等于分类概率分布，显示了任务类别为真的概率。**
  * ![image-20211217111455171](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20211217111455171.png)

##### 损失函数

最常用的损失函数**均方损失（MSELoss，通常用于回归问题）**、**对数损失（logLoss，常用于二分类问题）**和**交叉熵损失（CrossEntropyLoss，通常用于多分类问题）**

* 对数损失函数（logLoss，常用于二分类问题）：
  * **对数似然函数的基本思想是极大似然估计：**
    * 极大似然估计：就是一个事情已经发生了，那么就认为这事件发生的概率应该是最大的。似然函数就是这个概率，**基于现有数据确定参数从而最大化似然函数**，会对多个事情的概率进行连续乘法。**连续乘法中很多小数相乘的结果非常接近0**，而且任意数字发生变化，对最终结果的影响都很大。可以使用对数转换将连续乘法转换为连续加法。
    * 似然函数：**固定样本观察值x1,x2,...,xn，在O区中的可能范围内挑选出使得似然函数L(x1,x2,...,xn;o)达到最大的参数值**
      * ![image-20211217135223265](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20211217135223265.png)
      * 可以用sigmoid函数表示0-1中去1的概率，所以损失函数：
        * ![image-20211217135842958](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20211217135842958.png)
      * ![image-20211217135948860](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20211217135948860.png)
* 交叉熵损失函数：
  * 对数损失函数是针对二分类问题，**对于多分类问题，需要使用交叉熵损失函数**，对数损失函数也被称为二分类的交叉熵损失函数。
  * 多类别中每个类别都对应一个概率，所以k个类别的交叉熵损失函数公式：
    * ![image-20211217140542505](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20211217140542505.png)
    * 多分类中每个类的概率就需要用softmax激活函数得到，而二分类中的概率是用sigmoid激活函数得到。

##### 实际激活函数与损失函数

![image-20211217140925942](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20211217140925942.png)

##### Pytorch中优化器





